{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc36159a-42ae-4071-878a-8e67ff6e5603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "############################ Import des données ########################\n",
    "\n",
    "####### Pandas et Numpy.###########\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "########### Fonctions Spark. ##############\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame,Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import year,round,substring,udf,mean, col,countDistinct,sum,count,month,max,dayofyear,datediff,months_between,abs,min,isnull,avg\n",
    "\n",
    "################Plots.#################### \n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############### Regression logistiqu,multicolinéaire ###########\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "############ Apprentissage statistique (données manquantes, PCA, Diminution de variance)#################\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "############### Recherche au sein d'un string d'un autre string. #################\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff3a8ee-5549-41c1-9990-691fc99f5c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyse_resultat(objglm):\n",
    "    \"\"\"\n",
    "    Méthode pour déterminer si le modèle créé possède des limites avec notamment des Beta trop élevés.\n",
    "    Attributs:\n",
    "    ------------\n",
    "    objglm : objet issu d'un glm et d'un fit sur un jeu de données.\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    dict. Dictionnaire répertoriant la valeur de la log-vraisemblance ainsi que la valeur du vif pour chaque colonne.\n",
    "    \"\"\"\n",
    "\n",
    "    Cov=objglm.model.exog\n",
    "    i=0\n",
    "    VIF=[]\n",
    "    liste_noms=[]\n",
    "    dico_analyse={}\n",
    "    vif_col=[]\n",
    "    while i<Cov.shape[1]:\n",
    "        vif_col=variance_inflation_factor(Cov,i)\n",
    "        VIF.append(vif_col)\n",
    "        liste_noms.append(objglm.model.exog_names[i])\n",
    "        i=i+1\n",
    "    Table=pd.DataFrame()\n",
    "    Table[\"nom\"]=liste_noms\n",
    "    Table[\"VIF\"]=VIF\n",
    "    dico_analyse[\"VIF_colonnes\"]=Table\n",
    "    \"\"\"calculer la log vraisemblance, vérifier si na ou non.\"\"\"\n",
    "    loglikeli=objglm.llf\n",
    "    if loglikeli==np.nan:\n",
    "        reponse=False\n",
    "    else:\n",
    "        reponse=True\n",
    "    dico_analyse[\"Finie\"]=reponse\n",
    "    return(dico_analyse)\n",
    "\n",
    "def vars_a_traduire(df):\n",
    "    liste_vars=[]\n",
    "    i=0\n",
    "    for colonne in df.columns:\n",
    "        type_var=df.dtypes[i]\n",
    "        if type_var=='bool' or type_var==\"object\":\n",
    "            liste_vars.append(colonne)\n",
    "        i=i+1\n",
    "    return(liste_vars)\n",
    "\n",
    "def vars_explicatives(df):\n",
    "    \"\"\"\n",
    "    Permet dans le cas dans le cas de variables catégorielles et de variables numériques de retrouver la formule utilisée dans les régressions logistiques. \n",
    "    ----------------\n",
    "    Attributs:\n",
    "    -------\n",
    "    df : Dataframe de la librairie pandas. \n",
    "    ----------\n",
    "    Returns:\n",
    "    --------\n",
    "    chaîne : string. Correspond à l'équation des variables exogènes. \n",
    "    \"\"\"\n",
    "\n",
    "    chaine=\"\"\n",
    "    i=0\n",
    "    if isinstance(df,pd.DataFrame)==False:\n",
    "        df=pd.DataFrame({df.name:df.values})\n",
    "    for colonne in df.columns:\n",
    "        type_var=df.dtypes[i]\n",
    "        if type_var=='bool' or type_var==\"object\":\n",
    "            if i==0:\n",
    "                \"\"\"si on a une variable qui est catégorielle (une booléenne ou un object), on devra la transformer en facteur avec le C()\"\"\"\n",
    "                chaine=\"C(\"+colonne+\")\"\n",
    "            else:\n",
    "                chaine=chaine+\"+C(\"+colonne+\")\"\n",
    "        else:\n",
    "            if i==0:\n",
    "                chaine=colonne\n",
    "            else:\n",
    "                chaine=chaine+\"+\"+colonne\n",
    "        i=i+1\n",
    "    return(chaine)\n",
    "def description_influence(objlogit):\n",
    "    \"\"\" \n",
    "    Méthode permettant de connaître les effets des différentes variables sur le odd-ratio. \n",
    "    --------------\n",
    "    Attributs:\n",
    "    --------\n",
    "    objlogit : objet issu d'une régression logistique. \n",
    "    -------------\n",
    "    Returns:\n",
    "    --------\n",
    "    dico_description : dict. Les trois clés correspondent aux effets des variables, les noms des variables\n",
    "    ayant un effet positif ou à l'inverse négatif.\n",
    "    \"\"\"\n",
    "\n",
    "    dico_description={}\n",
    "    liste_positif=[]\n",
    "    liste_negatif=[]\n",
    "    dictionnaire_beta={}\n",
    "    liste_noms=objlogit.model.exog_names\n",
    "    i=0\n",
    "    mot=\"Début de la description des effets des variables.\\n ##################### \\n\"\n",
    "    while i<len(liste_noms):\n",
    "        dictionnaire_beta[str(liste_noms[i])]=objlogit.params[str(liste_noms[i])]\n",
    "        i=i+1\n",
    "    for key,value in dictionnaire_beta.items():\n",
    "        try :\n",
    "            valeur=math.exp(value)\n",
    "            mot=mot+\"La variable \"+key+ \" multiplie le odd_ratio par \"+str(valeur)+\".\\n\"\n",
    "            if math.exp(value)<1:\n",
    "                liste_negatif.append(key)\n",
    "            else:\n",
    "                liste_positif.append(key)\n",
    "        except:\n",
    "            \"\"\"dans le cas où le exponentiel est trop grand, on met dans la liste_positif.\"\"\"\n",
    "            mot=mot+\"La variable \"+key+ \" multiplie le odd_ratio par exp(\"+str(value.round(2))+\").\\n\"\n",
    "            liste_positif.append(key)\n",
    "        \n",
    "    mot=mot+\"######################\\n Fin de la description des effets.\"\n",
    "    dico_description[\"valeurs_beta\"]=dictionnaire_beta\n",
    "    dico_description[\"description_generale\"]=mot\n",
    "    dico_description[\"noms_lien_positif\"]=liste_positif\n",
    "    dico_description[\"noms_lien_negatif\"]=liste_negatif\n",
    "    return(dico_description)\n",
    "\n",
    "def  bilan(objlogit):\n",
    "    \"\"\"\n",
    "    Faire un bilan du modèle en fournissant les odd-ratios pour chaque ratio mais aussi la significativité des variables. \n",
    "    Attributs: \n",
    "    ----\n",
    "    objlogit : objet issu de la méthode d'un modèle glm (logistique).\n",
    "\n",
    "    Return :\n",
    "    ---------\n",
    "    Pandas Dataframe avec le nom de la variable, la valeur du beta, le odd-ratio et la significativité de la variable. \n",
    "    \"\"\"\n",
    "    \n",
    "    Bilan=pd.DataFrame()\n",
    "    Resultat_p=(objlogit.pvalues<0.05)\n",
    "    Significative=Resultat_p.values.tolist()\n",
    "    Nom_variable=Resultat_p.index.tolist()\n",
    "    Table_significative=pd.DataFrame({\"Nom_variable\":Nom_variable,\"Significativité\":Significative})\n",
    "    dico=description_influence(objlogit).get(\"valeurs_beta\")\n",
    "    Bilan[\"Nom_variable\"]=dico.keys()\n",
    "    Bilan[\"Valeur_beta\"]=dico.values()\n",
    "    Bilan[\"odd_ratio\"]=np.exp(Bilan[\"Valeur_beta\"]).round(2)\n",
    "    Bilan[\"Valeur_beta\"]=Bilan[\"Valeur_beta\"].round(2)\n",
    "    Bilan[\"Significativité\"]=Table_significative[\"Significativité\"]\n",
    "    return(Bilan)\n",
    "\n",
    "\n",
    "def predire_categorie(df,objet_glm):\n",
    "    \"\"\" Prédire la catégorie de la personne pour une nouvelle base de données. \n",
    "    -----------------\n",
    "    Attributs:\n",
    "    ----------\n",
    "    df : Dataframe pandas. Correspond à la base de données de test. \n",
    "    objet_glm. Objet issu d'un glm.fit (pour l'instant issu de SMF.fit)\n",
    "    ---------\n",
    "    Returns:\n",
    "    -----\n",
    "    Vecteur des prédictions. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \"\"\"Il peut arriver que le code ne fonctionne pas (Erreur : DataFrame has not attribute dtype). \"\"\"\n",
    "        predictions=objet_glm.predict(sm.add_constant(df))\n",
    "    except:\n",
    "        \"\"\"Si le transform ne fonctionne pas.\"\"\"\n",
    "        ensemble_cat=vars_a_traduire(df)\n",
    "        for colonne in ensemble_cat:\n",
    "            \"\"\"Pour les variables catégorielles, on a besoin de retrouver les bonnes noms de colonnes ainsi que les indicatrices.\"\"\"\n",
    "            var_factor=pd.get_dummies(Test[colonne])\n",
    "            for mod in var_factor.columns:\n",
    "                \"\"\"on adopte la manière de coder les variables catégorielles de smf\"\"\"\n",
    "                df[\"C(\"+str(colonne)+\")[T.\"+str(mod)+\"]\"]=var_factor[mod]\n",
    "        df=df.drop(columns=ensemble_cat)\n",
    "        \"\"\"on a plus besoin des anciennes colonnes \"\"\"\n",
    "        #Objet model venant de l'ancien glm. \n",
    "        \"\"\"Pour les variables catégorielles, on utilise toujours une modalité de référence. On a donc besoin de filtrer les colonnes.\"\"\"\n",
    "        colonnes_glm=objet_glm.model.exog_names\n",
    "        \"\"\"l'intersection est réalisée grâce à la méthode intersection du set.\"\"\"\n",
    "        liste_cols_final=list(set(df.columns).intersection(set(colonnes_glm)))\n",
    "        predictions=objet_glm.predict(sm.add_constant(df[liste_cols_final]),transform=False)\n",
    "    return(predictions)\n",
    "\n",
    "def selection_variables(df,methode,colonnes_x,colonne_y):\n",
    "    \"\"\" Donner les variables permettant de réduire le plus possible le BIC.  \n",
    "    Package utilisé : statmodels.\n",
    "    -----------------\n",
    "    Attributs:\n",
    "    ----------\n",
    "    df : Dataframe pandas. Correspond à la base de données d'appentissage.\n",
    "    methode : string. Correspond à la méthode utilisée pour obtenir le meilleur résultat. \n",
    "    colonnes_x: list. Liste des variables x utilisées. \n",
    "    colonne_y : nom de la variable à expliquer dans la base de données. \n",
    "    ---------\n",
    "    Returns:\n",
    "    -----\n",
    "    Noms des variables à utiliser. \n",
    "    \"\"\"\n",
    "    if methode==\"forward\":\n",
    "        var=\"1\"\n",
    "        liste_vars_utilisees=[]\n",
    "        bic=smf.glm(colonne_y+\"~\"+var,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "        arret=False\n",
    "        while arret==False:\n",
    "            if len(liste_vars_utilisees)==0:\n",
    "                nom_variable=\"1\"\n",
    "                for variable in colonnes_x:\n",
    "                    var_nouvelle=vars_explicatives(df[[variable]])\n",
    "                    bic_nouveau=smf.glm(colonne_y+\"~\"+var_nouvelle,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "                    if bic_nouveau<bic:\n",
    "                        bic=bic_nouveau\n",
    "                        nom_variable=variable\n",
    "                if nom_variable==\"1\":\n",
    "                    arret=True\n",
    "                else:\n",
    "                    liste_vars_utilisees.append(nom_variable)   \n",
    "            else:\n",
    "                nom_variable=liste_vars_utilisees[-1]\n",
    "                vars_exp_bic=vars_explicatives(df[liste_vars_utilisees])\n",
    "                bic=smf.glm(colonne_y+\"~\"+vars_exp_bic,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "                for variable in colonnes_x:\n",
    "                    if variable not in liste_vars_utilisees:\n",
    "                        vars_exp=vars_explicatives(df[liste_vars_utilisees])\n",
    "                        var_nouvelle=vars_explicatives(df[[variable]])\n",
    "                        bic_nouveau=smf.glm(colonne_y+\"~\"+vars_exp+\"+\"+var_nouvelle,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "                        \"\"\"Au moins une variable permet de réduire le bic donc on conserve son nom.\"\"\"\n",
    "                    else:\n",
    "                        bic_nouveau=smf.glm(colonne_y+\"~\"+var_nouvelle,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "                    if bic_nouveau<bic:\n",
    "                        bic=bic_nouveau\n",
    "                        nom_variable=variable\n",
    "                if nom_variable==liste_vars_utilisees[-1]:\n",
    "                    arret=True\n",
    "                else:\n",
    "                    liste_vars_utilisees.append(nom_variable)\n",
    "        return(liste_vars_utilisees)\n",
    "    \n",
    "    elif methode==\"backward\":\n",
    "        liste_vars_enlevees=[]\n",
    "        var=vars_explicatives(df[colonnes_x])\n",
    "        bic=smf.glm(colonne_y+\"~\"+var,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "        arret=False\n",
    "        while arret==False:\n",
    "            j=0\n",
    "            vars_exp_bic=vars_explicatives(df[colonnes_x].drop(columns=liste_vars_enlevees))\n",
    "            bic=smf.glm(colonne_y+\"~\"+vars_exp_bic,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "            for variable in colonnes_x:\n",
    "                if variable not in liste_vars_enlevees:\n",
    "                    vars_exp=vars_explicatives(df[colonnes_x].drop(columns=liste_vars_enlevees).drop(columns=variable))\n",
    "                    bic_nouveau=smf.glm(colonne_y+\"~\"+vars_exp,df,family=sm.families.Binomial()).fit().bic_deviance\n",
    "                    if bic_nouveau<bic:\n",
    "                        \"\"\" au moins une variable va pouvoir être enlevée.\"\"\"\n",
    "                        j=1\n",
    "                        bic=bic_nouveau\n",
    "                        nom_variable=variable\n",
    "            if j==0:\n",
    "                arret=True\n",
    "            else:\n",
    "                liste_vars_enlevees.append(nom_variable)\n",
    "        return(list(set(colonnes_x).symmetric_difference(set(liste_vars_enlevees))))\n",
    "\n",
    "def selection_variables_cv(df,methode,model, colonnes_x,colonne_y,K,NB,graine,seuil):\n",
    "    \"\"\"\n",
    "    Sélectionner les meilleures variables en n'utilisant plus le critère du BIC mais plutôt les performances au sein de la validation croisée. L'échantillon est celui d'apprentissage.\n",
    "    Il ne doit pas contenir l'échantillon de test. \n",
    "    Package utilisé : sklearn. \n",
    "\n",
    "    Attributs :\n",
    "    ------------\n",
    "    df : Pandas Dataframe. Echantillon d'appentissage.\n",
    "    methode : str. Méthode de la sélection de variables.\n",
    "    model : Méthode. Modèle utilisé.\n",
    "    colonnes_x : list de str. Variables explicatives.\n",
    "    colonne_y : str, nom de la variable à expliquer.\n",
    "    K : int. Nombre de blocs considérés à chaque tour.\n",
    "    NB : int. Nombre de répétitions. \n",
    "    graine : int. Graine aléatoire.\n",
    "    seuil : float. L'ajout d'une variable doit au moins augmenter le score d'au moins le seuil.\n",
    "\n",
    "    Return :\n",
    "    --------\n",
    "    list. Liste des colonnes contenant les variables sélectionnées.\n",
    "    \"\"\"\n",
    "    if methode==\"forward\": \n",
    "       liste_vars_utilisees=[]\n",
    "       arret=False\n",
    "       while arret==False:\n",
    "            \"\"\"On s'arrêtera si on ne parvient pas à augmenter le score avec l'ajout d'une nouvelle variable, d'où l'égalité entre nom_variables et nom_derniere_variable.\"\"\"\n",
    "            if len(liste_vars_utilisees)==0:\n",
    "                nom_derniere_variable=\" \"\n",
    "                Score_moyen=0\n",
    "            else:\n",
    "                nom_derniere_variable=liste_vars_utilisees[-1]\n",
    "                \"\"\" Variables explicatives, sélectionnées dans les tours précédents.\"\"\"\n",
    "                liste_variables=list(liste_vars_utilisees)\n",
    "                \"\"\" Variable à expliquer.\"\"\"\n",
    "                liste_variables.append(colonne_y)\n",
    "                print(liste_variables)\n",
    "                Score_moyen=calcul_score(df[liste_variables],model,K,NB,graine,colonne_y).mean()\n",
    "            nom_variable=nom_derniere_variable\n",
    "            for variable in colonnes_x:\n",
    "                modele=model\n",
    "                print(variable)\n",
    "                if variable not in liste_vars_utilisees:\n",
    "                    if len(liste_vars_utilisees)==0:\n",
    "                        table=pd.DataFrame({colonne_y:df[colonne_y].tolist(),variable:df[variable].tolist()})\n",
    "                        Liste_score=calcul_score(table,modele,K,NB,graine,colonne_y)\n",
    "                    else:\n",
    "                        liste=list(liste_vars_utilisees)\n",
    "                        liste.extend([variable,colonne_y])\n",
    "                        print(\"Les variables utilisées sont pour rappel \"+str(liste))\n",
    "                        table=pd.DataFrame(df[liste])\n",
    "                        Liste_score=calcul_score(table,modele,K,NB,graine,colonne_y)\n",
    "                    Nouveau_score=Liste_score.mean()\n",
    "                    if(Nouveau_score>=Score_moyen+seuil):\n",
    "                        \"\"\"Nom_derniere_variable ne vaut plus nom_variable si on a une variable donne un meilleur score moyen.\"\"\"\n",
    "                        nom_variable=variable\n",
    "                        Score_moyen=Nouveau_score\n",
    "                        print(str(Score_moyen)+ \" comme score avec l'ajout de \"+nom_variable)\n",
    "            if nom_derniere_variable==nom_variable:\n",
    "                arret=True\n",
    "            else:\n",
    "                print(nom_variable)\n",
    "                liste_vars_utilisees.append(nom_variable)\n",
    "       return(liste_vars_utilisees)\n",
    "    \n",
    "    elif methode==\"backward\":\n",
    "        liste_vars_enlevees=[]\n",
    "        modele=model\n",
    "        LISTE=list(colonnes_x)\n",
    "        LISTE.append(colonne_y)\n",
    "        Score_moyen=calcul_score(df[LISTE],modele,K,NB,graine,colonne_y).mean()\n",
    "        arret=False\n",
    "        while arret==False:\n",
    "            j=0\n",
    "            Score_moyen=calcul_score(df[LISTE].drop(columns=liste_vars_enlevees),modele,K,NB,graine,colonne_y).mean()\n",
    "            for variable in colonnes_x:\n",
    "                if variable not in liste_vars_enlevees:\n",
    "                    Nouveau_score=calcul_score(df[LISTE].drop(columns=liste_vars_enlevees).drop(columns=variable),modele,K,NB,graine,colonne_y).mean()\n",
    "                    if (Nouveau_score>=Score_moyen+seuil):\n",
    "                        \"\"\" au moins une variable va pouvoir être enlevée.\"\"\"\n",
    "                        j=1\n",
    "                        print(Nouveau_score)\n",
    "                        Score_moyen=Nouveau_score\n",
    "                        nom_variable=variable\n",
    "            if j==0:\n",
    "                arret=True\n",
    "            else:\n",
    "                liste_vars_enlevees.append(nom_variable)\n",
    "        return(list(set(colonnes_x).symmetric_difference(set(liste_vars_enlevees))))\n",
    "\n",
    "def calcul_score(table,model,K,NB,graine,colonne_y):\n",
    "    \"\"\"\n",
    "    Calculer le score du modèle pour les K blocs.\n",
    "    \n",
    "    Attributs :\n",
    "    -----\n",
    "    table : pandas Dataframe.\n",
    "    model : estimateur compatible avec Sklearn.\n",
    "    K: int. Nombre de blocs créés au sein de l'échantillon d'apprentissage.\n",
    "    NB : nombre de répétitions\n",
    "    graine : int. Graine aléatoire.\n",
    "    colonne_y : str. Nom de variable à expliquer.\n",
    "\n",
    "    Return: \n",
    "    ------\n",
    "    list de float. Correspond au score pour toutes les boucles.\n",
    "    \"\"\"\n",
    "    Table_explicatives=table.drop(columns=colonne_y)\n",
    "    cv=RepeatedKFold(n_splits=K,n_repeats=NB,random_state=graine)\n",
    "    cols_a_traduire=vars_a_traduire(Table_explicatives)\n",
    "    y=table[colonne_y]\n",
    "    if len(cols_a_traduire)>0:\n",
    "        table_pour_CV=Table_explicatives[list(set(Table_explicatives.columns).symmetric_difference(cols_a_traduire))]\n",
    "        for nom_col in cols_a_traduire:\n",
    "            colonne=pd.get_dummies(Table_explicatives[nom_col])\n",
    "            for modalite in colonne.columns:\n",
    "                table_pour_CV.loc[:,str(nom_col)+str(modalite)]=colonne[modalite]\n",
    "    else :\n",
    "        table_pour_CV=Table_explicatives\n",
    "    score_rate=cross_val_score(model,table_pour_CV,y,scoring='roc_auc',cv=cv)\n",
    "    return(score_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d74ecefc-7ea3-42fe-9a74-24f6378cc13b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############## Fonction PCA ############\n",
    "def fonction_pca(df,n):\n",
    "  \"\"\" \n",
    "  Permet de renvoyer la projection sur le plan factoriel des variables. Renvoie dans un dictionnaire la transformation du dataframe et les coordonnées des variables.\n",
    "  df : Dataframe (pandas). \n",
    "  n: int\n",
    "  ----------------------\n",
    "  Returns:\n",
    "  -------\n",
    "  liste_PCA : dict contenant la transformation du dataframe ainsi que les coordonnées des variables.\n",
    "  \"\"\"\n",
    "  df=pd.DataFrame(preprocessing.scale(df),columns=df.columns)\n",
    "  pca=PCA(n)\n",
    "  axes=pca.fit_transform(df)\n",
    "  for i in range(0,n):\n",
    "    df.loc[:,'PC'+str(i+1)]=axes[:,i]\n",
    "  pca.fit(df)\n",
    "  print(pca.explained_variance_ratio_)\n",
    "  #Prendre le maximum de chaque ligne avec .max(axis=1).\n",
    "  resultat=pca.components_.max(axis=1)\n",
    "  (fig, ax) = plt.subplots(figsize=(8, 8))\n",
    "  nb_variables=pca.components_.shape[1]\n",
    "  circle=plt.Circle((0,0),resultat[0]+0.10,facecolor=\"none\",edgecolor='k')\n",
    "  for i in range(0, pca.components_.shape[1]):\n",
    "      ax.arrow(0,\n",
    "              0,  # Start the arrow at the origin\n",
    "              #projection des variables sur le plan généré par les deux vecteurs propres. \n",
    "              pca.components_[0, i],  #0 for PC1\n",
    "              pca.components_[1, i],  #1 for PC2\n",
    "              head_width=0.1,\n",
    "              head_length=0.1)\n",
    "      ax.arrow(0,0,-resultat[0],0,head_width=0,\n",
    "              head_length=0)\n",
    "      ax.arrow(0,0,0,-resultat[1],head_width=0,\n",
    "              head_length=0)\n",
    "      ax.arrow(0,0,resultat[0],0,head_width=0,\n",
    "              head_length=0)\n",
    "      ax.arrow(0,0,0,resultat[1],head_width=0,\n",
    "              head_length=0)\n",
    "    #Mettre les noms des colonnes au bon endroit. \n",
    "      plt.text(pca.components_[0, i] + 0.02,\n",
    "              pca.components_[1, i] + 0.02,\n",
    "              df.columns.values[i])\n",
    "      ax.add_patch(circle)\n",
    " \n",
    "  print(pd.DataFrame(pca.components_,columns=df.columns))\n",
    "  dico={}\n",
    "  dico[\"donnees_modifiees\"]=axes\n",
    "  dico[\"coord\"]=pca.components_\n",
    "  return(dico)\n",
    "  \n",
    "######### fonction _acm dans le cas où on a des variables ordinales. #############\n",
    "# ########################### \n",
    "def fonction_ACM(data_base,nb_axes):\n",
    "    \"\"\"Permet de renvoyer la projection du dataframe sur les nb_axes, de visualiser la projection sur les deux premiers axes. Cela permet de renvoyer les coordonnées des variables. \n",
    "    ---------\n",
    "    Attributs:\n",
    "    ----\n",
    "    data_base : DataFrame (pandas). \n",
    "    nb_axes : int. Nombre d'axes factoriels considéré. \n",
    "    ---------\n",
    "    Returns :\n",
    "    ---- \n",
    "    dico_acm : dict. Contient les coordonnées des variables et la projection sur les nb_axes.\n",
    "    \"\"\"\n",
    "   \n",
    "    Table=pd.DataFrame()\n",
    "    for nom_col in data_base.columns:\n",
    "        colonne=pd.get_dummies(data_base[nom_col])\n",
    "        for modalite in colonne.columns:\n",
    "            Table.loc[:,nom_col+\"-\"+str(modalite)]=colonne[modalite]\n",
    "    data_normalise=pd.DataFrame(preprocessing.scale(Table),columns=Table.columns)\n",
    "    Acm_declare=PCA(nb_axes)\n",
    "    #L'ACM peut bloquer si on fractionne la base. \n",
    "    axes=Acm_declare.fit_transform(data_normalise)\n",
    "    #Creation des deux axes avec le fit_transform.\n",
    "    #On aura autant d'axes que de n_components dans la formule PCA(n_components).\n",
    "    #Le components donne les coordonnées sur le plan des variables. \n",
    "    for i in range(0,nb_axes):\n",
    "        data_normalise.loc[:,'PC'+str(i+1)]=axes[:,i].copy()\n",
    "    Acm_declare.fit(data_normalise)\n",
    "    print(Acm_declare.explained_variance_ratio_)\n",
    "    #Creation de la figure.\n",
    "    (figure, ax) = plt.subplots(figsize=(10, 10))\n",
    "    n=Acm_declare.components_.shape[1]\n",
    "    #Note : comme chez python pour les listes, pandas commence l'indentation à 0.\n",
    "    #[indice,:] pour prendre la liste des coordonnées pour la première dimension (: pour prendre toutes les colonnes).\n",
    "    maximum_x=Acm_declare.components_[0, n-2]\n",
    "    maximum_y=Acm_declare.components_[1, n-1]\n",
    "    circle=plt.Circle((0,0),maximum_x-0.05,facecolor=\"none\",edgecolor='k')\n",
    "    for j in range(0,Acm_declare.components_.shape[1]):\n",
    "        if j==n-1 or j==n-2:\n",
    "                ax.arrow(0,0,Acm_declare.components_[0, j], \n",
    "                Acm_declare.components_[1, j],head_width=0.1,\n",
    "                head_length=0.1)\n",
    "        else:\n",
    "            ax.plot(\n",
    "                    #projection des variables sur le plan généré par les deux vecteurs propres. \n",
    "                    Acm_declare.components_[0, j],  #0 for PC1\n",
    "                    Acm_declare.components_[1, j],'ro' #1 for PC2\n",
    "                    )\n",
    "            ax.arrow(0,0,-maximum_x,0,head_width=0,\n",
    "                  head_length=0)\n",
    "            ax.arrow(0,0,0,-maximum_y,head_width=0,\n",
    "                  head_length=0)\n",
    "        #Mettre les noms des colonnes au bon endroit. \n",
    "            plt.text(Acm_declare.components_[0, j] + 0.01,\n",
    "                     Acm_declare.components_[1, j] + 0.01,\n",
    "                     Table.columns.values[j])\n",
    "            plt.text(0.5,0.05,\"Axe 1 : \"+str(Acm_declare.explained_variance_ratio_[0].round(2)))\n",
    "            plt.text(0.05,0.5,\"Axe 2 : \"+str(Acm_declare.explained_variance_ratio_[1].round(2)))\n",
    "            plt.xlim((-maximum_x,maximum_x))\n",
    "            plt.ylim((-maximum_y,maximum_y))\n",
    "            ax.add_patch(circle)\n",
    "    dico_acm={}\n",
    "    dico_acm[\"donnees_modifiees\"]=axes\n",
    "    dico_acm[\"coord\"]=Acm_declare.components_\n",
    "    return(dico_acm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc2997a-0fa5-432f-9d38-2e77ae7d0709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transforme_prlda(df,NB,cols_,col_y):\n",
    "    \"\"\"\n",
    "    Permet de convertir pour la prédiction ou la création d'un modèle un dataframe. \n",
    "    Attributs:\n",
    "    -----------------\n",
    "    df : Dataframe pandas. \n",
    "    NB : nombre d'axes de l'ACM. \n",
    "    cols_: list. Les noms des variables étudiées. \n",
    "    Return:\n",
    "    -----------------\n",
    "    Dataframe prêt pour la LDA. \n",
    "    \"\"\"\n",
    "    df2=df[cols_]\n",
    "    liste_vars=vars_a_traduire(df2.drop(columns=[col_y]))\n",
    "    if len(liste_vars)>0:\n",
    "        Donnees_categorielles=df2[liste_vars]\n",
    "        Donnees_traduites=fonction_ACM(Donnees_categorielles,nb_axes=NB)[\"donnees_modifiees\"]\n",
    "        print(type(Donnees_traduites))\n",
    "        i=0\n",
    "        Data_pre_LDA=df2[vars_a_conserver(df2)]           \n",
    "        #### Traduction en numérique des variables cat.#####\n",
    "        while (i<NB):\n",
    "            Data_pre_LDA.loc[:,\"acm-\"+str(i+1)]=Donnees_traduites[:,i].copy()\n",
    "            i=i+1\n",
    "        Data_LDA=Data_pre_LDA[vars_a_conserver(Data_pre_LDA)].copy()\n",
    "        sc=StandardScaler()\n",
    "        Data_LDA=sc.fit_transform(Data_LDA)\n",
    "        return(Data_LDA)\n",
    "    else:\n",
    "        sc=StandardScaler()\n",
    "        Data_LDA=sc.fit_transform(df2)\n",
    "        return(Data_LDA)\n",
    "\n",
    "def modele_lda(df,NB,cols_,col_y):\n",
    "    \"\"\" [[[[ Ce n'est qu'un essai avec une méthode alternative.]]]] \n",
    "    Réaliser une analyse linéaire discriminante. \n",
    "    -------\n",
    "    Attribut:\n",
    "    ---------\n",
    "    df : Dataframe pandas. \n",
    "    NB : nombre d'axes de l'ACM. \n",
    "    cols_: list. Liste des noms des colonnes. \n",
    "    col_y. Str. Nom de la variable à prédire. \n",
    "\n",
    "    Returns:\n",
    "    ------\n",
    "    Renvoie un dictionnaire pour résumer le modèle. \n",
    "    \"\"\"\n",
    "    Data_LDA=transforme_prlda(df,NB,cols_,col_y)\n",
    "    x=Data_LDA\n",
    "    y=df[col_y].to_numpy()\n",
    "    LDA=LinearDiscriminantAnalysis()\n",
    "    LDA.fit(x,y)\n",
    "    dico_resultat_lda={}\n",
    "    dico_resultat_lda[\"modele\"]=LDA\n",
    "    dico_resultat_lda[\"x_apprentissage\"]=x\n",
    "    dico_resultat_lda[\"y_apprentissage\"]=y\n",
    "    return(dico_resultat_lda)\n",
    "\n",
    "def projection_lda(df,objet_lda):\n",
    "    \"\"\"\n",
    "    Projeter les individus sur le plan scalaire avec les couleurs associées à Y.\n",
    "    Attributs:\n",
    "    -----------\n",
    "    df : pandas Dataframe. Données présentes (X,Y). Données de test ou d'apprentissage. \n",
    "    objet_lda : objet issu de modèle lda.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Visuel dans un repère avec les individus.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71aabc05-b7ff-4fc8-88d4-34962e48c74b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def traiter_multicolinéarité(objglm,data,colonne_y,colonnes_x,valeur_vif_max=5):\n",
    "    \"\"\"\n",
    "    La méthode permet de retirer les variables qui ont un vif strictement supérieur à un certaine valeur. Notons que la valeur du vif pour la constante n'est pas importante.\n",
    "    Comme le VIF peut rapidement être diminué, on prend variable par variable. \n",
    "    Attributs :\n",
    "    ----------\n",
    "    objglm : objet issu d'un smf.glm, d'une régression logistique. Cela renvoie à un smf.glm(...).fit()\n",
    "    data : DataFrame pandas correspondant à l'échantillon d'apprentissage.\n",
    "    colonne_y : str. Nom de la variable à prédire.\n",
    "    colonnes_x : str. Nom des variables explicatives.\n",
    "    valeur_vif_max : float. Il correspond à la valeur maximale autorisée de VIF.\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    dictionnaire. Trois valeurs: la valeur du vif actuel, un objet glm avec le problème traité et le nom des colonnes.\n",
    "    \"\"\"\n",
    "    \"\"\"Liste des colonnes à enlever.\"\"\"\n",
    "    dictionnaire_vif={}\n",
    "    Table_vif=analyse_resultat(objglm).get(\"VIF_colonnes\")\n",
    "    sous_table=Table_vif[(Table_vif[\"VIF\"]>5)&(Table_vif[\"nom\"]!=\"Intercept\")]\n",
    "    if sous_table[\"nom\"].count()>0:\n",
    "        liste_drop=[]\n",
    "        condition=False\n",
    "        while condition==False:\n",
    "            maximum=sous_table[\"VIF\"].max()\n",
    "            Nom_maximum=sous_table[sous_table[\"VIF\"]==maximum][\"nom\"].to_list()[0]\n",
    "            if re.search('C(.+)[T.+]',Nom_maximum):\n",
    "                \"\"\"Si on a une variable catégorielle, nous devons retrouver dans le data la bonne colonne.\"\"\"\n",
    "                for colonne in data.columns:\n",
    "                    if colonne in Nom_maximum:\n",
    "                        liste_drop.append(colonne)\n",
    "                        break\n",
    "            else:\n",
    "                liste_drop.append(Nom_maximum)\n",
    "            Nouvelle_liste_variable=list(set(colonnes_x).symmetric_difference(set(liste_drop)))\n",
    "            formule_x=vars_explicatives(data[Nouvelle_liste_variable])\n",
    "            nouvglm=smf.glm(colonne_y+\"~\"+formule_x,data,family=sm.families.Binomial()).fit()\n",
    "            nouv_table=analyse_resultat(nouvglm).get(\"VIF_colonnes\")\n",
    "            sous_table=nouv_table[(nouv_table[\"VIF\"]>5)&(nouv_table[\"nom\"]!=\"Intercept\")]\n",
    "            if sous_table[\"nom\"].count()==0:\n",
    "                condition=True\n",
    "        dictionnaire_vif[\"GLM\"]=nouvglm\n",
    "        dictionnaire_vif[\"resultat_vif\"]=nouv_table\n",
    "        dictionnaire_vif[\"nom_colonnes\"]=list(set(colonnes_x).intersection(set(Nouvelle_liste_variable)))\n",
    "    else:\n",
    "        print(\"Aucun changement à réaliser.\")\n",
    "        dictionnaire_vif[\"GLM\"]=objglm\n",
    "        dictionnaire_vif[\"resultat_vif\"]=Table_vif\n",
    "        dictionnaire_vif[\"nom_colonnes\"]=colonnes_x\n",
    "    return(dictionnaire_vif)\n",
    "\n",
    "\n",
    "def courbe_ROC(nom_col_pred,nom_col_realite,table,valeur,couleur=\"blue\"):\n",
    "    \"\"\"\n",
    "    Construire la courbe ROC. \n",
    "    -------------\n",
    "    Attributs:\n",
    "    -------\n",
    "    nom_col_pred : string. nom dans la table \"table\" de la prédiction. \n",
    "    nom_col_realite : string. Nom dans la table \"table\" de la réalité (vrai y).\n",
    "    table : DataFrame. Table de test du modèle. \n",
    "    valeur : int. Valeur pour laquelle Y est dit positive. \n",
    "    couleur : str. Couleur de la droite.\n",
    "    Return:\n",
    "    --------\n",
    "    Graphique de la courbe ROC\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(table[nom_col_realite], table[nom_col_pred], pos_label=valeur)\n",
    "    \"\"\" fpr \"Increasing false positive rates\". Proportion de y_chapeau=1 pour les individus n'utilisant pas les téléservices. \"\"\"\n",
    "    \"\"\"  tpr \"Increasing true positive rates\". Proportion de y_chapeau=1 pour les individus utilisant les téléservices. \"\"\"\n",
    "    \"\"\" Dans notre cas, le tpr sera parfois un vecteur de 1.\"\"\"\n",
    "    (figure, ax) = plt.subplots(figsize=(7, 7))\n",
    "    point0=np.asarray([0,0])\n",
    "    point1=np.asarray([1,1]) \n",
    "    plt.plot(fpr,tpr,couleur)\n",
    "    #Droite du hasard.\n",
    "    plt.axline((0,0),(1,1))\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "def liste_courbe_ROC(liste_cols_pred,nom_col_realite,table,valeur,liste_couleurs):\n",
    "    \"\"\"\n",
    "    Représenter plusieurs courbes ROC dans le même repère. \n",
    "    Attributs:\n",
    "    ----------\n",
    "    liste_cols_pred: list d'éléments string.  Chaque chaîne de caractères est une colonne issue de la prédiction d'un modèle. \n",
    "    nom_colm_realite : string. Nom de la colonne de la variable expliquée dans la table. \n",
    "    table : Dataframe pandas. table de test fournissant le X et le Y. \n",
    "    liste_couelurs : list de string. Correspond à une liste de couleurs utilisée pour distinguer chaque modèle. \n",
    "\n",
    "    Return:\n",
    "    ---------\n",
    "    Graphique récapitulant la performance de chaque modèle en terme de sensibilité, spécificité. \n",
    "    \"\"\"\n",
    "    i=0\n",
    "    (figure, ax) = plt.subplots(figsize=(7, 7))\n",
    "    for prediction in liste_cols_pred:\n",
    "        couleur=liste_couleurs[i]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(table[nom_col_realite], table[prediction], pos_label=valeur)\n",
    "        plt.plot(fpr,tpr,couleur,label=prediction)\n",
    "        plt.legend()\n",
    "        plt.title(\"Courbe ROC sur le nouvel échantillon\")\n",
    "        i=i+1\n",
    "    plt.axline((0,0),(1,1),label=\"hasard\")\n",
    "    plt.legend()\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb01089f-7baa-4758-86dd-c9cff0c29fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convertir_pourSklearn(df,drop_one_mod=False):\n",
    "    \"\"\" Convertir une table dataframe en une table pour sklearn. L'option d'enlever une modalité est disponible. Cela est nécessaire pour retrouver des résultats proches de ceux \n",
    "    de statmodels.\n",
    "    Paramètres :\n",
    "    -----------\n",
    "    df : Pandas dataframe. \n",
    "    drop_one_mod : Bool. Indique si on enlève une modalité. \n",
    "    \"\"\"\n",
    "\n",
    "    Table_explicatives=df\n",
    "    cols_a_traduire=vars_a_traduire(Table_explicatives)\n",
    "    if len(cols_a_traduire)>0:\n",
    "        table_sk=Table_explicatives[list(set(Table_explicatives.columns).symmetric_difference(cols_a_traduire))]\n",
    "        for nom_col in cols_a_traduire:\n",
    "            colonne=pd.get_dummies(Table_explicatives[nom_col],drop_first=drop_one_mod)\n",
    "            for modalite in colonne.columns:\n",
    "                table_sk.loc[:,str(nom_col)+str(modalite)]=colonne[modalite]\n",
    "    else :\n",
    "        table_sk=Table_explicatives\n",
    "    return(table_sk)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fichiers_codes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
